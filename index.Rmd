---
title: "Decoding KPOP vs. KRnB"
output:
    flexdashboard::flex_dashboard:
      storyboard: true
      theme: flatly
author: Ananta Wibisono
      
date: "2024-02-14"
---

```{r asdfsadf, echo=FALSE}
library(spotifyr)
krnb <- get_playlist_audio_features("", "5k7upSNskiHxxf6DSFnHD1")
kpop <- get_playlist_audio_features("", "25tn6jO4XMfbtSv9ukYvYt")
krnb_general <- get_playlist_audio_features("", "2TJHOlrTUoLMHs9CTyxN9n")
krnb_general_50 <- head(krnb_general, 50)
kpop_general <- get_playlist_audio_features("", "37i9dQZF1DX9tPFwDMOaN1")

kpop_general_sorted <- kpop_general[order(kpop_general$energy), ]

# Remove the 5 lowest energy tracks
kpop_general_49 <- kpop_general_sorted[-(1:5), ]



warr <- get_track_audio_features(c("1ZEFYW6nPEvIcsIvymgsLk", "3DBKc4ioGnMQLlbGQcFDIO"))

library(ggplot2)
library(plotly)
library(dplyr)
library(tibble)
library(compmus)
library(tidyr)
library(purrr)

# Add a column to each dataframe to specify the genre for labeling on the plot
krnb$genre <- "krnb"
kpop$genre <- "kpop"
krnb_general_50$genre <- "krnb"
kpop_general$genre <- "kpop"

# Combine the dataframes
combined_data <- rbind(krnb, kpop, krnb_general_50, kpop_general)

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

# combined_data
# colnames(combined_data)
```
Tempogram
========================================

Column {.tabset}
-------------------------------------
### Tempo Histogram

```{r tempo, echo=FALSE}
# Split dataframe based on different values in the "Value" column
df_kpop <- combined_data[combined_data$genre == "kpop", ]
df_krnb <- combined_data[combined_data$genre == "krnb", ]

# Plot with mean lines
histies2 <- ggplot() +
  geom_histogram(data = df_krnb, aes(x = tempo, fill = "KRnB", text = paste("Song:", track.name, tempo)), alpha = 0.5) +
  geom_histogram(data = df_kpop, aes(x = tempo, fill = "K-POP", text = paste("Song:", track.name, tempo)), alpha = 0.5) +
  scale_fill_manual(name = "Genre", values = c("KRnB" = "purple", "K-POP" = "cyan" )) + theme(legend.position = "right") + labs(title = "Tempo histogram for KRnB and KPOP")

histies2 <- ggplotly(histies2, tooltip = "text")

histies2
```

### Lowest Tempo: KPOP

```{r tempogram, echo=FALSE}
graveola <- get_tidy_audio_analysis("1c6kkrWnpy68eYDfBdxNtF")

graveola |>
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic() + labs(title = "Shopper - IU")

```


### Highest Tempo: KR&B

```{r tempogram2, echo=FALSE}
graveola <- get_tidy_audio_analysis("4iZIvaus7v4wSjFvdF83NA")

graveola |>
  tempogram(window_size = 8, hop_size = 1, cyclic = FALSE) |>
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic() + labs(title = "FLYING HIGH WITH U - Vinxen")

```

Column {data-width=275}
-------------------------------------

**Key Histogram**


As you can see from the histogram, most K-pop songs tend to hover around 125 beats per minute (BPM), while KR&B songs often have a spike around 85 BPM. However, it's common for songs in both genres to stray from these averages. One standout outlier among KR&B tracks is "FLYING HIGH WITH U" by Vinxen. On the other hand, "shopper" by IU is one of the K-pop songs with a lower BPM. You can find these outliers by hovering over the plot, of which the tooltip shows the title and the BPM of each song. 

Now, let's take a closer look at the tempogram analysis of these songs.

**Shopper - IU**

The tempogram displayed here is a non-cyclical one, meaning it may show multiples of the main BPM. In this case, it's interesting because Spotify tagged the song with a BPM of 75, but the tempogram indicates a BPM of around 200 for most of the song, with only a small dot around 75 BPM at the beginning. However, when you listen to the song, it starts to make sense. At the beginning, there's a section with a distinct clapping noise that seems to follow a rhythm of 75 BPM, uninterrupted by other instruments except for the vocals. This is followed by a beat that might sound like 200 BPM but is actually closer to 75 BPM to the human ear.

**FLYING HIGH WITH U - Vinxen**

Interestingly, the tempogram for this song does align with the BPM provided by Spotify, which is around 210 BPM. The tempogram even indicates activity for multiples of 210 BPM. However, despite this numerical match, when you actually listen to the song, it doesn't sound like it has such a fast tempo. This discrepancy might be due to the difference in tempo between the drums and the vocals. While the drums might be driving the high BPM detected by the tempogram, the vocals maintain a slower pace, resulting in an overall perception of a slower tempo. 



Key Analysis
========================================

Column {.tabset}
-------------------------------------
### Key Histogram.

```{r key_histogram plot, echo=FALSE}
# Dkey <- combined_data %>%
#   arrange(desc(track.popularity)) %>%
#   filter(key_name == 'D')
# 
# # View the filtered data
# print(Dkey[, c("track.name", "key_name", "track.popularity", "track.album.name")])
# 
# Gkey <- combined_data %>%
#   arrange(desc(track.popularity)) %>%
#   filter(key_name == 'G')
# 
# # View the filtered data
# print(Gkey[, c("track.name", "key_name", "track.popularity", "track.album.name")])


ggplot(combined_data, aes(x = key_name, fill = genre)) + 
  geom_bar() +
  facet_wrap(~genre, scales = "free") +
  labs(title = "Histogram of Keys by Genre", x = "key")

```

### D key song
```{r chordogram1 plot, echo=FALSE}
twenty_five <-
  get_tidy_audio_analysis("3m5PgWSClkZ44vdFmPiqpq") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

twenty_five |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(title = "Keygram: Yes or No", x = "Time (s)", y = "")
```

### G key song
```{r chordogram2 plot, echo=FALSE}
twenty_five <-
  get_tidy_audio_analysis("4RiudH8RehvLLrk8uNgIdR") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

twenty_five |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() + 
  geom_vline(xintercept = 89.55, color = "red", size = 0.65) +
  geom_vline(xintercept = 153, color = "red", size = 0.65) +
  labs(title = "Keygram: Nightwalker", x = "Time (s)", y = "")
```


Column {data-width=275}
-------------------------------------

**Key Histogram**

In the genre of KPOP, the most prevalent keys are C sharp, F, and G, as indicated by the plots. Conversely, in KRnB, the histogram reveals that the A and G key is favored among other keys.

In KPOP, the prevalence of C sharp, F, and G keys mirrors their common usage in Western pop music, suggesting a shared musical influence between the two genres. Conversely, within KRnB, the dominance of the A and G keys underscores the unique tonal preferences characteristic of Korean R&B music. These trends underscore the dynamic interplay between cultural influences and genre-specific characteristics, illustrating how musical styles can both converge and diverge across different contexts.

**Keygrams for songs in different keys**

Now lets look at the keygrams.

In looking at the keygrams of the most popular songs in the corpus, some interesting patterns emerge regarding their tonal centers.

For instance, when looking at the song in the key of D, we notice that the confidence for D minor key dominates, which suggests a strong association with that tonality. But what's really interesting is that the confidence for C sharp minor key pops up even more frequently than for D minor. This might be because of some connection between D minor and C sharp minor keys. The keys share some of the same notes, which could explain why C sharp minor shows up so prominently alongside D minor.

Now, the keygram of the song in G key is a bit more straightforward. It's clear that both G minor and major keys rule the roost throughout the song, indicating a strong focus on G as the tonal center. Also interesting to note: there are a couple of moments in the song where the keygram isn't too sure about which key it's in, besides the usual uncertainty at the beginning and end of songs. These moments of uncertainty could signal shifts in harmony or brief changes in tonality within the song.


Self similarity matrices
========================================

Column {data-width=750}
-------------------------------------

### Self similarity matrix

```{r self plot, echo=FALSE}
bzt <-
  get_tidy_audio_analysis("1ZEFYW6nPEvIcsIvymgsLk") |> # Change URI.
  compmus_align(bars, segments) |>                     # Change `bars`
  select(bars) |>                                      #   in all three
  unnest(bars) |>                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) |>
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )
```

```{r self plot2, echo=FALSE}
  bzt |>
  compmus_self_similarity(timbre, "cosine") |> 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(x = "", y = "") + labs(title = "WA-R-R - Colde")
```

Column {data-width=250}
-------------------------------------
**Description**

no description yet 

this is my fav song tho btw

Chroma and Timbre plots
========================================

Column {.tabset}
-------------------------------------

### Lowest Energy Song Chromagram

```{r chromagram, echo=FALSE}


wood1 <-
  get_tidy_audio_analysis("2bdVgAQgosGUJoViVDNeOV") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

wood1 |>
  mutate(pitches = map(pitches, compmus_normalise, "chebyshev")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c() +
  labs(title = "LIMBO - Keshi")
```

### Highest Energy Song Chromagram

```{r chromagram2, echo=FALSE}
library(tidyr)
library(purrr)

wood2 <-
  get_tidy_audio_analysis("0RDqNCRBGrSegk16Avfzuq") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)

wood2 |>
  mutate(pitches = map(pitches, compmus_normalise, "chebyshev")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c() + labs(title = "Talk that Talk - TWICE")
```



Column {data-width=150}
-------------------------------------

**Chromagram Comparison**

I selected two songs from the corpus based on their energy and loudness scores provided by Spotify. One of the songs is "LIMBO" by keshi, which scores low in both energy and loudness. The other song is "Talk That Talk" by TWICE, which scores high in both energy and loudness.

When comparing the chromagrams of these songs, the differences are quite apparent. "Talk That Talk" is notably higher in tempo compared to "LIMBO."

In terms of musical composition, "Talk That Talk" employs a wider range of notes and chords whereas "LIMBO" maintains a simpler structure with a clear emphasis on the G sharp/A flat note.

Regarding the data from the Spotify API, it seems less confident in identifying the notes used in "Talk That Talk" compared to "LIMBO." This could be attributed to the greater variability in the composition of "Talk That Talk."

Overall, these observations underscore how songs can differ significantly in their energy, and musical complexity, as perceived by listeners.



Overview
========================================

Column {data-width=500}
-------------------------------------

### Corpus Description

**Introduction**

I've put together a collection of playlists that represent my taste in music, particularly focusing on KRnB and KPOP genres. These playlists consist of four categories: two are tailored to my personal preferences in KRnB and KPOP, while the other two feature more general KRnB and KPOP tracks. The selection of these playlists is based on my deep love for KRnB and KPOP music.

The personal playlists are curated by Spotify, using my listening history to tailor the content to my tastes. On the other hand, the broader KRnB and KPOP playlists are popular among a wider audience.

With this collection, I aim to analyze and understand the differences between KRnB and KPOP. I'm also interested in exploring why I prefer certain KPOP and KRnB tracks over others. This analysis will help me gain insights into my music preferences and the unique characteristics of these genres.



Column {data-width=500}
-------------------------------------

![]("C:\Users\anant\Downloads\overview.png")


Corpus Analysis
========================================



Column {.tabset}
-------------------------------------

### Energy Distribution

```{r difference in tempo, echo=FALSE}
# Calculate mean values
krnb_mean <- mean(krnb$energy)
kpop_mean <- mean(kpop$energy)
gen_krnb_mean <- mean(krnb_general_50$energy)
gen_kpop_mean <- mean(kpop_general_49$energy)

# Plot with mean lines
histies <- ggplot() +
  geom_histogram(data = krnb, aes(x = energy, fill = "Personal Korean R&B Mix"), alpha = 0.5) +
  geom_histogram(data = kpop, aes(x = energy, fill = "Personal K-POP Mix"), alpha = 0.5) +
  geom_histogram(data = krnb_general_50, aes(x = energy, fill = "General Korean R&B"), alpha = 0.5) +
  geom_histogram(data = kpop_general_49, aes(x = energy, fill = "General K-POP"), alpha = 0.5) +
  geom_vline(xintercept = krnb_mean, color = "green", linetype = "twodash", size = 0.5) + 
  geom_vline(xintercept = kpop_mean, color = "red", linetype = "twodash", size = 0.5) +    
  geom_vline(xintercept = gen_krnb_mean, color = "blue", linetype = "twodash", size = 0.5) + 
  geom_vline(xintercept = gen_kpop_mean, color = "yellow", linetype = "twodash", size = 0.5) +  
  scale_fill_manual(name = "Genre", values = c("Personal Korean R&B Mix" = "green", "Personal K-POP Mix" = "red", "General Korean R&B" = "blue", "General K-POP" = "yellow" )) +
  theme(legend.position = "right") + labs(title = "Energy Difference between Krnb and Kpop") + theme_classic() 

histies <- ggplotly(histies)

histies
```

### Acousticness Comparison

```{r timbre plot, echo=FALSE}
library(Hmisc)

# Create a boxplot for acousticness
boxplot_acousticness <- ggplot(combined_data, aes(x = genre, y = acousticness, fill = genre)) +
  geom_boxplot() +
  labs(title = "Comparison of Acousticness between krnb and kpop", x = "Genre", y = "Acousticness") +
  scale_fill_manual(values = c("krnb" = "purple", "kpop" = "green")) +
  theme_minimal()


boxplot_acousticness <- ggplotly(boxplot_acousticness)

boxplot_acousticness


```

```{r f, echo=FALSE}
# Find the row with the maximum acousticness value
max_acousticness_row <- combined_data[which.min(combined_data$acousticness), ]

# Print the row
print(max_acousticness_row)
```

Column {data-width=250}
-------------------------------------

**Energy Distribution**

In this plot I pulled in data of 4 different playlists: 

1. My Personal KrnB playlist
2. My Personal Kpop playlist
3. A popular KRnB playlist
4. A popular KPOP playlist

Let's first look at the personal playlists by selecting that option in the plot legend.

What we see is that, on average, KPOP songs tend to have higher energy levels compared to KRnB. But there's a catch – there's some overlap between the two genres. This means that a KRnB track could have the energy level typical of a KPOP song, and vice versa. It goes to show that energy level is just one factor among many that define a song's genre.

Now, when we switch over to the general playlists, things look different. The songs seem more spread out across energy levels compared to the personal playlists. This suggests that personally, I might have a specific energy level in mind when picking KRnB or KPOP songs for my playlists. It's all about personal taste and what vibes with me the most.

**Acousticness Comparison**


Looking at the plot, it's clear that KRnB songs tend to have higher acousticness compared to KPOP tunes. There's just a small sliver of overlap between the acousticness of KRnB and KPOP tracks. Also, notice how the range of acousticness for KRnB songs is wider than that of KPOP. This suggests that KRnB artists have more leeway in using different instruments compared to the more structured sound of KPOP.



Relation between loudness and energy
========================================


Column {.tabset}
-------------------------------------

### Relation Plot

```{r energy plot, echo=FALSE}
corr_plot <- ggplot() +
  geom_point(data = krnb, aes(energy, loudness, color = "krnb", size = valence, text = track.name), alpha = 0.7) +
  geom_point(data = kpop, aes(energy, loudness, color = "kpop", size = valence, text = track.name), alpha = 0.7) +
  # Calculate and add a linear regression line through all points
  geom_smooth(data = rbind(krnb, kpop), aes(energy, loudness), method = "lm", se = FALSE, color = "red", alpha = 0.4) +
  scale_color_manual(values = c("krnb" = "blue", "kpop" = "green"),
                     labels = c("kpop", "krnb"),
                     name = "Genres") + 
  labs(title = "Energy vs Loudness with Valence as size")

corr_plot <- ggplotly(corr_plot, tooltip = c("text"))

corr_plot
```




Column {data-width=250}
-------------------------------------



**Description**

In the plot, you can clearly see how energy and loudness are linked, especially with that regression line slicing through the data points. There's also this faint boundary line that seems to separate KPOP and KRnB songs. Oh, and there's this one standout: the KPOP hit "Cupid" by Fifty Fifty, chilling in the KRnB zone for loudness and energy. Just hover over the point to check it out!


